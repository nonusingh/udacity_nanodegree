{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling Open Street Map data with MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Area\n",
    "#### Austin, TX, United States\n",
    "https://mapzen.com/data/metro-extracts/metro/austin_texas/\n",
    "\n",
    "https://www.openstreetmap.org/relation/113314\n",
    "\n",
    "Austin is where I did my first internship so it is special to me. I am also familiar with the street names so I will be able to audit it better. I downloaded osm file from mapzen.com which consisted of data from the shaded area in the map below. It is over 1.4 GB(uncompressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"img/SizeM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auditing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the huge size of the data, I will load a smaller section of data as a sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T12:04:14.394752",
     "start_time": "2017-03-04T12:04:14.375590"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T12:04:16.758675",
     "start_time": "2017-03-04T12:04:16.755569"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OSM_FILE = \"austin_texas.osm\"\n",
    "SAMPLE_FILE = \"sample.osm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T11:12:01.826213",
     "start_time": "2017-03-04T11:11:50.360683"
    },
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 10\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag Reference: http://stackoverflow.com/questions/\n",
    "    3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write('<osm>\\n  ')\n",
    "\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write('</osm>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the sample data, lets parse one tag at a time with ElementTree and count the number of top level tags. Iterative parsing is utilized for this as data is too large to process on the complete document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T12:10:52.031621",
     "start_time": "2017-03-04T12:04:21.507175"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1,\n",
      " 'member': 20369,\n",
      " 'nd': 7020265,\n",
      " 'node': 6386286,\n",
      " 'osm': 1,\n",
      " 'relation': 2396,\n",
      " 'tag': 2387514,\n",
      " 'way': 669483}\n"
     ]
    }
   ],
   "source": [
    "def count_tags(filename):\n",
    "    \"\"\"count tags in filename.\n",
    "    \n",
    "    Init 1 in dict if the key not exist, increment otherwise.\"\"\"\n",
    "    tags = {}\n",
    "    for ev, elem in ET.iterparse(filename):\n",
    "        tag = elem.tag\n",
    "        if tag not in tags.keys():\n",
    "            tags[tag] = 1\n",
    "        else:\n",
    "            tags[tag] += 1\n",
    "    return tags\n",
    "\n",
    "\n",
    "tags = count_tags(OSM_FILE)\n",
    "pprint.pprint(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, Let us explore the data some more to check for potential problems in the data. I have created regular expressions to check for certain patterns in the tag \"k=addr:\", the function 'key_type' will count of each of the four tag categories in a dictionary:\n",
    "  \"lower\", for tags that contain only lowercase letters and are valid,\n",
    "  \"lower_colon\", for otherwise valid tags with a colon in their names,\n",
    "  \"problemchars\", for tags with problematic characters, and\n",
    "  \"other\", for other tags that do not fall into the other three categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T12:17:52.094479",
     "start_time": "2017-03-04T12:10:58.083163"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 1307962, 'lower_colon': 1067521, 'other': 12030, 'problemchars': 1}\n"
     ]
    }
   ],
   "source": [
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        for tag in element.iter('tag'):\n",
    "            k = tag.get('k')\n",
    "            if lower.search(k):\n",
    "                keys['lower'] += 1\n",
    "            elif lower_colon.search(k):\n",
    "                keys['lower_colon'] += 1\n",
    "            elif problemchars.search(k):\n",
    "                keys['problemchars'] += 1\n",
    "            else:\n",
    "                keys['other'] += 1\n",
    "    return keys\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "    return keys\n",
    "\n",
    "\n",
    "keys = process_map(OSM_FILE)\n",
    "pprint.pprint(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's now find out how many unique users have contributed to the map in this osm, following code gives us 1260 different unique users who have contributed to the street map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T12:28:34.256336",
     "start_time": "2017-03-04T12:21:25.961036"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1260"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_map_users(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        for e in element:\n",
    "            if 'uid' in e.attrib:\n",
    "                users.add(e.attrib['uid'])\n",
    "\n",
    "    return users\n",
    "\n",
    "\n",
    "users = process_map_users(OSM_FILE)\n",
    "len(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auditing Street Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to create a set of expected values for the street names. The next function is a regex to match the last token in a string optionally ending with a period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T12:28:41.950481",
     "start_time": "2017-03-04T12:28:41.940027"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "street_type_reg = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "expected_street_types = [\"Avenue\", \"Boulevard\", \"Commons\", \"Court\",\n",
    "                         \"Drive\",\"Lane\", \"Parkway\", \"Place\", \"Road\",\n",
    "                         \"Square\", \"Street\", \"Trail\", \"Way\", \"Vista\",\n",
    "                         \"Terrace\",\"Trace\",\"Valley\", \"View\", \"Walk\",\n",
    "                         \"Run\",\"Ridge\",\"Row\",\"Point\",\"Plaza\",\"Path\",\n",
    "                         \"Pass\",\"Park\",\"Overlook\",\"Meadows\",\"Loop\",\n",
    "                         \"Hollow\",\"Hill\",\"Highway\",\"Expressway\",\"Cove\",\n",
    "                         \"Crossing\",\"Creek\",\"Circle\",\"Canyon\",\"Bend\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function: audit_street_type will search for the above regex , If there is a match and it's not in our list of expected street types, it will add the street_name to the street_type dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T12:28:45.292982",
     "start_time": "2017-03-04T12:28:45.287797"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def audit_street_type(street_types, street_name,\n",
    "                      regex, expected_street_types):\n",
    "    m = regex.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected_street_types:\n",
    "            street_types[street_type].add(street_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is_street_name determines if an element contains an attribute k=\"addr:street\" and returns it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T12:28:49.051171",
     "start_time": "2017-03-04T12:28:49.047554"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, an audit function to iterate over way and node tags to print out all the various street types found in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T12:28:51.217043",
     "start_time": "2017-03-04T12:28:51.208546"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def audit(osmfile, regex):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "\n",
    "    # iteratively parse the mapping xml\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        # iterate 'tag' tags within 'node' and 'way' tags\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'],\n",
    "                                      regex, expected_street_types)\n",
    "\n",
    "    return street_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us print some of the street types now using pprint and depth=5 as it is a very long list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T12:36:00.415168",
     "start_time": "2017-03-04T12:28:54.161427"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'100': set(['Avery Ranch Blvd Building A #100',\n",
      "             'Jollyville Road Suite 100',\n",
      "             'Old Jollyville Road, Suite 100',\n",
      "             'RM 2222 Unit #100']),\n",
      " '101': set(['4207 James Casey st #101']),\n",
      " '104': set(['11410 Century Oaks Terrace Suite #104', 'S 1st St, Suite 104']),\n",
      " '1100': set(['Farm-to-Market Road 1100']),\n",
      " '117': set(['County Road 117']),\n",
      " '12': set(['Ranch to Market Road 12']),\n",
      " '120': set(['Building B Suite 120']),\n",
      " '129': set(['County Road 129']),\n",
      " '130': set(['Highway 130']),\n",
      " '1327': set(['FM 1327', 'Farm-to-Market Road 1327']),\n",
      " '138': set(['County Road 138']),\n",
      " '140': set(['S IH 35 Frontage Rd #140']),\n",
      " '1431': set(['Farm-to-Market Road 1431', 'Old Farm-to-Market 1431']),\n",
      " '150': set(['Farm-to-Market Road 150',\n",
      "             'IH-35 South, #150',\n",
      "             'Metric Boulevard #150',\n",
      "             'Ranch-to-Market Road 150']),\n",
      " '1625': set(['Farm-to-Market Road 1625']),\n",
      " '1626': set(['F.M. 1626', 'FM 1626', 'Farm-to-Market Road 1626']),\n",
      " '163': set(['Bee Cave Road Suite 163']),\n",
      " '170': set(['County Road 170']),\n",
      " '1805': set(['N Interstate 35, Suite 1805']),\n",
      " '1825': set(['FM 1825']),\n",
      " '1826': set(['Farm To Market Road 1826', 'Ranch to Market Road 1826']),\n",
      " '183': set(['Highway 183',\n",
      "             'N HWY 183',\n",
      "             'U.S. 183',\n",
      "             'US 183',\n",
      "             'US Highway 183',\n",
      "             'United States Highway 183']),\n",
      " '1869': set(['Ranch-to-Market Road 1869']),\n",
      " '2': set(['6800 Burnet Rd #2']),\n",
      " '200N': set(['Burnet Road #200N']),\n",
      " '203': set(['West Ben White Boulevard #203',\n",
      "             'West Ben White Boulevard, #203']),\n",
      " '213': set(['Executive Center Drive Suite 213']),\n",
      " '2222': set(['Ranch to Market Road 2222']),\n",
      " '2243': set(['Old FM 2243']),\n",
      " '2244': set(['RM 2244']),\n",
      " '260': set(['S Interstate 35, #260']),\n",
      " '2769': set(['Farm-to-Market Road 2769']),\n",
      " '280': set(['County Road 280']),\n",
      " '290': set(['C R 290',\n",
      "             'County Road 290',\n",
      "             'E Hwy 290',\n",
      "             'East Highway 290',\n",
      "             '"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 2000 exceeded with 18552 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "street_types = audit(OSM_FILE, street_type_reg)\n",
    "pprint.pprint(dict(street_types), depth = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems in Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) First Problem I noticed is abbreviated street types ex St for Street, Ct for Court, Ave for Avenue etc. Next function will be for mapping these to their unabbreviated form so they are consistent and standardized, I will also standardize N,E,W,S to North, East, West, South"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T12:38:41.253957",
     "start_time": "2017-03-04T12:38:41.246752"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update(name, mapping): \n",
    "    words = name.split()\n",
    "    for w in range(len(words)):\n",
    "        if words[w] in mapping:\n",
    "            if words[w].lower() not in ['suite', 'ste.', 'ste']: \n",
    "                # For example, don't update 'Suite E' to 'Suite East'\n",
    "                words[w] = mapping[words[w]] \n",
    "                name = \" \".join(words)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T12:38:42.530868",
     "start_time": "2017-03-04T12:38:42.516877"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "street_type_mapping = {'Ave':'Avenue','Ave.':'Avenue','Avene':'Avenue',\n",
    "                       'Blvd' : 'Boulevard','Blvd.' : 'Boulevard',\n",
    "                       'Cv' : 'Cove',\n",
    "                       'Dr'   : 'Drive','Dr.' : 'Drive', \n",
    "                       'hwy':'Highway','Hwy':'Highway','HWY':'Highway',\n",
    "                       'Ln' : 'Lane',\n",
    "                       'Pkwy' : 'Parkway',\n",
    "                       'Rd'   : 'Road',\n",
    "                       'St':'Street','St.':'Street','street':'Street',\n",
    "                       'Ovlk' : 'Overlook',\n",
    "                       'way': 'Way',\n",
    "                       'N' : 'North','N.': 'North',\n",
    "                       'S' : 'South','S.': 'South',\n",
    "                       'E' : 'East','E.': 'East',\n",
    "                       'W': 'West','W.': 'West',\n",
    "                       'IH35':'Interstate Highway 35',\n",
    "                       'IH 35':'Interstate Highway 35',\n",
    "                       'I 35':'Interstate Highway 35',\n",
    "                       'I-35':'Interstate Highway 35'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us search street types again and replace abbreviations with full standardized street types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T12:38:49.218223",
     "start_time": "2017-03-04T12:38:48.894165"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merimac => Merimac\n",
      "Clara Van => Clara Van\n",
      "Capri => Capri\n",
      "Chelsea Moor => Chelsea Moor\n",
      "Royal Birkdale Ovlk => Royal Birkdale Overlook\n",
      "Apache => Apache\n",
      "Farm-to-Market Road 812 => Farm-to-Market Road 812\n",
      "Bee Cave Road Suite 163 => Bee Cave Road Suite 163\n",
      "Adventurer => Adventurer\n",
      "Affirmed => Affirmed\n",
      "West 35th Street Cutoff => West 35th Street Cutoff\n",
      "Ferguson Cutoff => Ferguson Cutoff\n",
      "House Wren => House Wren\n",
      "N I-35 Suite 298 => North Interstate Highway 35 Suite 298\n",
      "Melody => Melody\n",
      "East Highway 290 => East Highway 290\n",
      "Highway 290 => Highway 290\n",
      "W. Highway 290 => West Highway 290\n",
      "East Hwy 290 => East Highway 290\n",
      "C R 290 => C R 290\n",
      "West Highway 290 => West Highway 290\n",
      "W Hwy 290 => West Highway 290\n",
      "U.S. 290 => U.S. 290\n",
      "West US Highway 290 => West US Highway 290\n",
      "E Hwy 290 => East Highway 290\n",
      "US Highway 290 => US Highway 290\n",
      "County Road 290 => County Road 290\n",
      "W HWY 290 => West Highway 290\n",
      "Helios Way, Bldg 2, Suite 290 => Helios Way, Bldg 2, Suite 290\n",
      "US 290 => US 290\n",
      "Pony Chase => Pony Chase\n",
      "Pecan Chase => Pecan Chase\n",
      "Mustang Chase => Mustang Chase\n",
      "Glen Rose Chase => Glen Rose Chase\n",
      "Shetland Chase => Shetland Chase\n",
      "Wild Basin Lodge => Wild Basin Lodge\n",
      "Palm Harborway => Palm Harborway\n",
      "south church street => south church Street\n",
      "East main street => East main Street\n",
      "South 1st street => South 1st Street\n",
      "White House street => White House Street\n",
      "Carlos G Parker Boulevard Northwest => Carlos G Parker Boulevard Northwest\n",
      "Raven Caw pass => Raven Caw pass\n",
      "Applegate Drive East => Applegate Drive East\n",
      "Mockingbird Lane East => Mockingbird Lane East\n",
      "Alpine Road East => Alpine Road East\n",
      "Brenham Street East => Brenham Street East\n",
      "Basin Ledge East => Basin Ledge East\n",
      "Hwy 290 East => Highway 290 East\n",
      "Ledgeway East => Ledgeway East\n",
      "Main Street East => Main Street East\n",
      "Black Locust Drive East => Black Locust Drive East\n",
      "Rutland Village East => Rutland Village East\n",
      "US 290 East => US 290 East\n",
      "Covington Drive East => Covington Drive East\n",
      "Colorado Drive East => Colorado Drive East\n",
      "Cypress Point East"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 2000 exceeded with 11523 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for street_type, ways in street_types.iteritems():\n",
    "    for name in ways:\n",
    "        better_name = update(name, street_type_mapping)\n",
    "        print name, \"=>\", better_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above the mapping has been applied correctly to give full forms for cardinal directions and Ln, Dr, etc. Also updated IH-35/I-35 etc to 'Interstate Highway 35'(major highway in austin connecting San Antonio and Dallas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) For Postal Codes I used a similar approach as in the Street Name Cleaning. I converted them to standard 5 digit postal codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T12:38:58.486861",
     "start_time": "2017-03-04T12:38:58.460743"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zip_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "zip_types = defaultdict(set)\n",
    "expected_zip = [\"73301\",\"73344\",\"76574\",\"78602\",\"78610\",\"78612\",\n",
    "                \"78613\",\"78615\",\"78616\",\"78617\",\"78619\",\"78620\",\n",
    "                \"78621\",\"78626\",\"78628\",\"78634\",\"78640\",\"78641\",\n",
    "                \"78642\",\"78644\",\"78645\",\"78646\",\"78652\",\"78653\",\n",
    "                \"78654\",\"78656\",\"78660\",\"78663\",\"78664\",\"78665\",\n",
    "                \"78666\",\"78669\",\"78676\",\"78680\",\"78681\",\"78682\",\n",
    "                \"78691\",\"78701\",\"78702\",\"78703\",\"78704\",\"78705\",\n",
    "                \"78712\",\"78717\",\"78719\",\"78721\",\"78722\",\"78723\",\n",
    "                \"78724\",\"78725\",\"78726\",\"78727\",\"78728\",\"78729\",\n",
    "                \"78730\",\"78731\",\"78732\",\"78733\",\"78734\",\"78735\",\n",
    "                \"78736\",\"78737\",\"78738\",\"78739\",\"78741\",\"78742\",\n",
    "                \"78744\",\"78745\",\"78746\",\"78747\",\"78748\",\"78749\",\n",
    "                \"78750\",\"78751\",\"78752\",\"78753\",\"78754\",\"78756\",\n",
    "                \"78757\",\"78758\",\"78759\",\"78957\"]\n",
    "def audit_zip_codes(zip_types, zip_name, regex, expected_zip):\n",
    "    m = regex.search(zip_name)\n",
    "    if m:\n",
    "        zip_type = m.group()\n",
    "        if zip_type not in expected_zip:\n",
    "             zip_types[zip_type].add(zip_name)\n",
    "def is_zip_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "def audit(filename, regex):\n",
    "    for event, elem in ET.iterparse(filename, events=(\"start\",)):\n",
    "        if elem.tag == \"way\" or elem.tag == \"node\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_zip_name(tag):\n",
    "                    audit_zip_codes(zip_types, tag.attrib['v'], \n",
    "                                    regex, expected_zip)\n",
    "    return zip_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T12:46:00.206179",
     "start_time": "2017-03-04T12:39:00.403953"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'76574-4649': set(['76574-4649']),\n",
      " '78613-2277': set(['78613-2277']),\n",
      " u'78626\\u200e': set([u'78626\\u200e']),\n",
      " '78640-4520': set(['78640-4520']),\n",
      " '78640-6137': set(['78640-6137']),\n",
      " '78704-5639': set(['78704-5639']),\n",
      " '78704-7205': set(['78704-7205']),\n",
      " '78724-1199': set(['78724-1199']),\n",
      " '78728-1275': set(['78728-1275']),\n",
      " '78753-4150': set(['78753-4150']),\n",
      " '78754-5701': set(['78754-5701']),\n",
      " '78758-7008': set(['78758-7008']),\n",
      " '78758-7013': set(['78758-7013']),\n",
      " '78759-3504': set(['78759-3504']),\n",
      " 'Texas': set(['Texas']),\n",
      " 'tx': set(['tx'])}\n"
     ]
    }
   ],
   "source": [
    "audit(OSM_FILE, zip_type_re)\n",
    "pprint.pprint(dict(zip_types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To standardize the zipcodes, I will keep the first 5 digits in the postal code and drop the digits after the hyphen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T12:52:48.120986",
     "start_time": "2017-03-04T12:52:48.106526"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_zip(postcode):\n",
    "    return postcode.split(\"-\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T12:53:14.024729",
     "start_time": "2017-03-04T12:53:14.012240"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78759-3504 => 78759\n",
      "tx => tx\n",
      "78626‎ => 78626‎\n",
      "78613-2277 => 78613\n",
      "76574-4649 => 76574\n",
      "78754-5701 => 78754\n",
      "78724-1199 => 78724\n",
      "78704-7205 => 78704\n",
      "Texas => Texas\n",
      "78758-7008 => 78758\n",
      "78728-1275 => 78728\n",
      "78753-4150 => 78753\n",
      "78640-4520 => 78640\n",
      "78758-7013 => 78758\n",
      "78640-6137 => 78640\n",
      "78704-5639 => 78704\n"
     ]
    }
   ],
   "source": [
    "for zip_type, ways in zip_types.iteritems():\n",
    "    for postal in ways:\n",
    "        better_zip = update_zip(postal)\n",
    "        print postal, \"=>\", better_zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for Mongo DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T12:53:17.966070",
     "start_time": "2017-03-04T12:53:17.909454"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "CREATED = [\"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "\n",
    "def shape_element(element):\n",
    "    node = {}    \n",
    "    if element.tag == \"node\" or element.tag == \"way\" :\n",
    "        node['type'] = element.tag\n",
    "\n",
    "        # Parse attributes\n",
    "        for attrib in element.attrib:\n",
    "\n",
    "            # Data creation details\n",
    "            if attrib in CREATED:\n",
    "                if 'created' not in node:\n",
    "                    node['created'] = {}\n",
    "                if attrib == 'timestamp':\n",
    "                    node['created'][attrib] = dt.strptime(element.attrib[attrib], \n",
    "                                                                '%Y-%m-%dT%H:%M:%SZ')\n",
    "                else:\n",
    "                    node['created'][attrib] = element.get(attrib)\n",
    "\n",
    "            # Parse location\n",
    "            if attrib in ['lat', 'lon']:\n",
    "                lat = float(element.attrib.get('lat'))\n",
    "                lon = float(element.attrib.get('lon'))\n",
    "                node['pos'] = [lat, lon]\n",
    "\n",
    "            # Parse the rest of attributes\n",
    "            else:\n",
    "                node[attrib] = element.attrib.get(attrib)\n",
    "\n",
    "        # Process tags\n",
    "        for tag in element.iter('tag'):\n",
    "            key   = tag.attrib['k']\n",
    "            value = tag.attrib['v']\n",
    "            if not problemchars.search(key):\n",
    "\n",
    "                # Tags with single colon and beginning with addr\n",
    "                if lower_colon.search(key) and key.find('addr') == 0:\n",
    "                    if 'address' not in node:\n",
    "                        node['address'] = {}\n",
    "                    sub_attr = key.split(':')[1]\n",
    "                    if is_street_name(tag):\n",
    "                        # Do some cleaning\n",
    "                        better_name = update(tag.attrib['v'],\n",
    "                        street_type_mapping)\n",
    "                        node['address'][sub_attr] = better_name\n",
    "                    if key == 'postcode' or key == 'addr:postcode':\n",
    "                        node['address'][sub_attr]=update_zip(tag.attrib['v'])\n",
    "                    else:    \n",
    "                        node['address'][sub_attr] = value\n",
    "                \n",
    "\n",
    "                # All other tags that don't begin with \"addr\"\n",
    "                elif not key.find('addr') == 0:\n",
    "                    if key not in node:\n",
    "                        node[key] = value\n",
    "                else:\n",
    "                    node[\"tag:\" + key] = value\n",
    "\n",
    "        # Process nodes\n",
    "        for nd in element.iter('nd'):\n",
    "            if 'node_refs' not in node:\n",
    "                node['node_refs'] = []\n",
    "            node['node_refs'].append(nd.attrib['ref'])\n",
    "\n",
    "        return node\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T13:13:59.028100",
     "start_time": "2017-03-04T12:58:13.171783"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "import json\n",
    "from bson import json_util\n",
    "\n",
    "def process_map(file_in, pretty = False):\n",
    "    file_out = \"{0}.json\".format(file_in)    \n",
    "    with open(file_out, \"wb\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2,\n",
    "                                        default=json_util.default)+\"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el,default=json_util.default)+\"\\n\")\n",
    "\n",
    "process_map(OSM_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T13:14:47.296724",
     "start_time": "2017-03-04T13:14:47.291670"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The downloaded file is 1414.558803 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print 'The downloaded file is {} MB'.format(os.path.getsize(OSM_FILE)\n",
    "                                            /1.0e6) \n",
    "# convert from bytes to megabytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T13:14:49.486687",
     "start_time": "2017-03-04T13:14:49.482059"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The json file is 2543.250995 MB\n"
     ]
    }
   ],
   "source": [
    "print 'The json file is {} MB'.format(os.path.getsize(OSM_FILE + \".json\")\n",
    "                                      /1.0e6) \n",
    "# convert from bytes to megabytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Street Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T13:21:52.911120",
     "start_time": "2017-03-04T13:14:52.881650"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326698"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osm_file = open(OSM_FILE, \"r\")\n",
    "address_count = 0\n",
    "\n",
    "for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "    if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "        for tag in elem.iter(\"tag\"): \n",
    "            if is_street_name(tag):\n",
    "                address_count += 1\n",
    "\n",
    "address_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T13:29:57.705088",
     "start_time": "2017-03-04T13:29:57.116499"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import signal\n",
    "import subprocess\n",
    "\n",
    "# The os.setsid() is passed in the argument preexec_fn so\n",
    "# it's run after the fork() and before  exec() to run the shell.\n",
    "pro = subprocess.Popen('mongod', preexec_fn = os.setsid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to database with PyMongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T13:30:04.398533",
     "start_time": "2017-03-04T13:30:04.321030"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "db_name = 'openstreetmap'\n",
    "\n",
    "# Connect to Mongo DB\n",
    "client = MongoClient('localhost:27017')\n",
    "# Database 'openstreetmap' will be created if it does not exist.\n",
    "db = client[db_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T13:37:26.080784",
     "start_time": "2017-03-04T13:32:04.444254"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: mongoimport -h 127.0.0.1:27017 --db openstreetmap --collection austin_texas --file /Users/tarunparmar/Nonu/austin_texas.osm.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build mongoimport command\n",
    "collection = OSM_FILE[:OSM_FILE.find('.')]\n",
    "working_directory = '/Users/tarunparmar/Nonu/'\n",
    "json_file = OSM_FILE + '.json'\n",
    "\n",
    "mongoimport_cmd = 'mongoimport -h 127.0.0.1:27017 ' + \\\n",
    "                  '--db ' + db_name + \\\n",
    "                  ' --collection ' + collection + \\\n",
    "                  ' --file ' + working_directory + json_file\n",
    "\n",
    "# Before importing, drop collection if it exists (i.e. a re-run)\n",
    "if collection in db.collection_names():\n",
    "    print 'Dropping collection: ' + collection\n",
    "    db[collection].drop()\n",
    "\n",
    "# Execute the command\n",
    "print 'Executing: ' + mongoimport_cmd\n",
    "subprocess.call(mongoimport_cmd.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T13:37:58.661700",
     "start_time": "2017-03-04T13:37:58.657686"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "austin_texas = db[collection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T13:38:02.155834",
     "start_time": "2017-03-04T13:38:02.151009"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), u'openstreetmap'), u'austin_texas')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "austin_texas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T13:38:04.638998",
     "start_time": "2017-03-04T13:38:04.633589"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7055769"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "austin_texas.find().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Unique Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T13:38:14.070563",
     "start_time": "2017-03-04T13:38:06.650089"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1249"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(austin_texas.distinct('created.user'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1249 vs 1260 users we began with, probably due to loss of data in shape_element function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Nodes and Ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T13:38:23.546110",
     "start_time": "2017-03-04T13:38:19.045126"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6386286"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.austin_texas.find( {\"type\":\"node\"} ).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T13:38:30.367775",
     "start_time": "2017-03-04T13:38:25.888781"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "669483"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.austin_texas.find( {\"type\":\"way\"} ).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 5 Contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T13:39:09.187286",
     "start_time": "2017-03-04T13:38:58.151689"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': u'patisilva_atxbuildings', u'count': 2742450}\n",
      "{u'_id': u'ccjjmartin_atxbuildings', u'count': 1300433}\n",
      "{u'_id': u'ccjjmartin__atxbuildings', u'count': 940002}\n",
      "{u'_id': u'wilsaj_atxbuildings', u'count': 358812}\n",
      "{u'_id': u'jseppi_atxbuildings', u'count': 300855}\n"
     ]
    }
   ],
   "source": [
    "pipeline = [{'$group': {'_id': '$created.user','count': {'$sum' : 1}}},{'$sort': {'count' : -1}},{'$limit': 5}]\n",
    "\n",
    "def aggregate(db, pipeline):\n",
    "    result = db.austin_texas.aggregate(pipeline)\n",
    "    #pprint.pprint(result)\n",
    "    return result\n",
    "\n",
    "result = aggregate(db, pipeline)\n",
    "\n",
    "for document in result:\n",
    "    pprint.pprint(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "atx buildings works on tracking and documenting the community effort to import the 2013 building footprints and address points datasets from the City of Austin Data Portal. Andy Wilson(wilsaj_atxbuildings), \n",
    "John Clary(johnclary_atxbuildings), James Seppi(jseppi_atxbuildings), Chris Martin(ccjjmartin_atxbuildings), Kelvin Thompson(kkt_atxbuildings), Jonathan Pa(jonathan pa_atxbuildings) and Pati Silva(patisilva_atxbuildings) are all participants of this project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of users appearing only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T13:39:44.585156",
     "start_time": "2017-03-04T13:39:33.574252"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': 1, u'num_users': 280}\n"
     ]
    }
   ],
   "source": [
    "pipeline = [{\"$group\":{\"_id\":\"$created.user\", \"count\":{\"$sum\":1}}}, {\"$group\":{\"_id\":\"$count\", \"num_users\":{\"$sum\":1}}}, {\"$sort\":{\"_id\":1}}, {\"$limit\":1}]\n",
    "\n",
    "def aggregate(db, pipeline):\n",
    "    result = db.austin_texas.aggregate(pipeline)\n",
    "    #pprint.pprint(result)\n",
    "    return result\n",
    "\n",
    "result = aggregate(db, pipeline)\n",
    "\n",
    "for document in result:\n",
    "    pprint.pprint(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Postal Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T13:39:51.650933",
     "start_time": "2017-03-04T13:39:47.943192"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': u'78645', u'count': 10883}\n",
      "{u'_id': u'78734', u'count': 5607}\n",
      "{u'_id': u'78653', u'count': 3544}\n",
      "{u'_id': u'78660', u'count': 3518}\n",
      "{u'_id': u'78669', u'count': 3189}\n",
      "{u'_id': u'78641', u'count': 2870}\n",
      "{u'_id': u'78704', u'count': 2466}\n",
      "{u'_id': u'78746', u'count': 2450}\n",
      "{u'_id': u'78759', u'count': 2093}\n",
      "{u'_id': u'78738', u'count': 1939}\n"
     ]
    }
   ],
   "source": [
    "pipeline =[{'$match': {'address.postcode': {'$exists': 1}}},{'$group': {'_id': '$address.postcode','count': {'$sum': 1}}}, {'$sort': {'count': -1}},{'$limit': 10}]\n",
    "def aggregate(db, pipeline):\n",
    "    result = db.austin_texas.aggregate(pipeline)\n",
    "    #pprint.pprint(result)\n",
    "    return result\n",
    "\n",
    "result = aggregate(db, pipeline)\n",
    "\n",
    "for document in result:\n",
    "    pprint.pprint(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the postal codes have been cleaned up by our script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T13:39:59.657709",
     "start_time": "2017-03-04T13:39:55.598496"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': u'North Lamar Boulevard', u'count': 679}\n",
      "{u'_id': u'Burnet Road', u'count': 558}\n",
      "{u'_id': u'North Interstate Highway 35 Service Road', u'count': 551}\n",
      "{u'_id': u'Ranch Road 620', u'count': 494}\n",
      "{u'_id': u'South Congress Avenue', u'count': 482}\n",
      "{u'_id': u'Shoal Creek Boulevard', u'count': 445}\n",
      "{u'_id': u'South 1st Street', u'count': 425}\n",
      "{u'_id': u'Guadalupe Street', u'count': 397}\n",
      "{u'_id': u'Manchaca Road', u'count': 391}\n",
      "{u'_id': u'Cameron Road', u'count': 369}\n"
     ]
    }
   ],
   "source": [
    "pipeline =[{'$match': {'address.street': {'$exists': 1}}},{'$group': {'_id': '$address.street','count': {'$sum': 1}}}, {'$sort': {'count': -1}},{'$limit': 10}]\n",
    "def aggregate(db, pipeline):\n",
    "    result = db.austin_texas.aggregate(pipeline)\n",
    "    #pprint.pprint(result)\n",
    "    return result\n",
    "\n",
    "result = aggregate(db, pipeline)\n",
    "\n",
    "for document in result:\n",
    "    pprint.pprint(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the street names have been cleaned up by our script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cities in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T13:40:09.862865",
     "start_time": "2017-03-04T13:40:02.881885"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': None, u'count': 7052097}\n",
      "{u'_id': u'Austin', u'count': 3127}\n",
      "{u'_id': u'Round Rock', u'count': 119}\n",
      "{u'_id': u'Kyle', u'count': 62}\n",
      "{u'_id': u'Austin, TX', u'count': 50}\n",
      "{u'_id': u'Cedar Park', u'count': 40}\n",
      "{u'_id': u'Leander', u'count': 39}\n",
      "{u'_id': u'Pflugerville', u'count': 36}\n",
      "{u'_id': u'Buda', u'count': 26}\n",
      "{u'_id': u'Georgetown', u'count': 14}\n"
     ]
    }
   ],
   "source": [
    "pipeline =[{\"$group\":{\"_id\":\"$address.city\", \"count\":{\"$sum\":1}}}, {\"$sort\":{\"count\": -1}},{'$limit': 10}]\n",
    "def aggregate(db, pipeline):\n",
    "    result = db.austin_texas.aggregate(pipeline)\n",
    "    #pprint.pprint(result)\n",
    "    return result\n",
    "\n",
    "result = aggregate(db, pipeline)\n",
    "\n",
    "for document in result:\n",
    "    pprint.pprint(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of addresses had 'None' in their city name. \n",
    "\n",
    "Other than Austin, TX, the map includes the neighboring areas like Kyle, Buda, Round Rock etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional data exploration using MongoDB queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T13:40:20.420074",
     "start_time": "2017-03-04T13:40:13.500205"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': None, u'count': 7047676}\n",
      "{u'_id': u'parking', u'count': 2198}\n",
      "{u'_id': u'restaurant', u'count': 805}\n",
      "{u'_id': u'waste_basket', u'count': 602}\n",
      "{u'_id': u'fast_food', u'count': 596}\n",
      "{u'_id': u'school', u'count': 559}\n",
      "{u'_id': u'place_of_worship', u'count': 515}\n",
      "{u'_id': u'fuel', u'count': 441}\n",
      "{u'_id': u'bench', u'count': 360}\n",
      "{u'_id': u'shelter', u'count': 239}\n"
     ]
    }
   ],
   "source": [
    "pipeline = [{\"$group\":{\"_id\":\"$amenity\", \"count\":{\"$sum\":1}}}, {\"$sort\":{\"count\": -1}},{'$limit': 10}]\n",
    "def aggregate(db, pipeline):\n",
    "    result = db.austin_texas.aggregate(pipeline)\n",
    "    #pprint.pprint(result)\n",
    "    return result\n",
    "\n",
    "result = aggregate(db, pipeline)\n",
    "\n",
    "for document in result:\n",
    "    pprint.pprint(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top Amenities are Parking, Restaurants and Waste Baskets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Religion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T13:40:28.398654",
     "start_time": "2017-03-04T13:40:24.861765"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': u'christian', u'count': 465}\n",
      "{u'_id': None, u'count': 32}\n",
      "{u'_id': u'buddhist', u'count': 6}\n",
      "{u'_id': u'muslim', u'count': 3}\n",
      "{u'_id': u'jewish', u'count': 3}\n"
     ]
    }
   ],
   "source": [
    "pipeline = [{\"$match\":{\"amenity\":{\"$exists\":1}, \"amenity\":\"place_of_worship\"}},{\"$group\":{\"_id\":\"$religion\", \"count\":{\"$sum\":1}}},{\"$sort\":{\"count\":-1}}, {\"$limit\":5}]\n",
    "def aggregate(db, pipeline):\n",
    "    result = db.austin_texas.aggregate(pipeline)\n",
    "    #pprint.pprint(result)\n",
    "    return result\n",
    "\n",
    "result = aggregate(db, pipeline)\n",
    "\n",
    "for document in result:\n",
    "    pprint.pprint(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top Religion is 'Christianity' with 465 places of worship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T13:40:42.651436",
     "start_time": "2017-03-04T13:40:39.097349"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': None, u'count': 21}\n",
      "{u'_id': u\"Chili's\", u'count': 10}\n",
      "{u'_id': u'IHOP', u'count': 6}\n",
      "{u'_id': u\"Applebee's\", u'count': 5}\n",
      "{u'_id': u\"Denny's\", u'count': 5}\n",
      "{u'_id': u'Pizza Hut', u'count': 4}\n",
      "{u'_id': u'Baby Acapulco', u'count': 4}\n",
      "{u'_id': u'Chipotle Mexican Grill', u'count': 4}\n",
      "{u'_id': u'Olive Garden', u'count': 3}\n",
      "{u'_id': u\"Chuy's\", u'count': 3}\n"
     ]
    }
   ],
   "source": [
    "pipeline =[{'$match': {'amenity': 'restaurant'}},{'$group': {'_id': '$name','count': {'$sum': 1}}},{'$sort': {'count': -1}},{'$limit': 10}]\n",
    "def aggregate(db, pipeline):\n",
    "    result = db.austin_texas.aggregate(pipeline)\n",
    "    #pprint.pprint(result)\n",
    "    return result\n",
    "\n",
    "result = aggregate(db, pipeline)\n",
    "\n",
    "for document in result:\n",
    "    pprint.pprint(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Popular Cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-04T13:40:52.149544",
     "start_time": "2017-03-04T13:40:48.589383"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': None, u'count': 405}\n",
      "{u'_id': u'mexican', u'count': 74}\n",
      "{u'_id': u'american', u'count': 35}\n",
      "{u'_id': u'pizza', u'count': 33}\n",
      "{u'_id': u'chinese', u'count': 23}\n"
     ]
    }
   ],
   "source": [
    "pipeline = [{\"$match\":{\"amenity\":{\"$exists\":1}, \"amenity\":\"restaurant\"}}, {\"$group\":{\"_id\":\"$cuisine\", \"count\":{\"$sum\":1}}},{\"$sort\":{\"count\":-1}}, {\"$limit\":5}]\n",
    "def aggregate(db, pipeline):\n",
    "    result = db.austin_texas.aggregate(pipeline)\n",
    "    #pprint.pprint(result)\n",
    "    return result\n",
    "\n",
    "result = aggregate(db, pipeline)\n",
    "\n",
    "for document in result:\n",
    "    pprint.pprint(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Popular cuisines include Mexican, American and Pizza- no surprise there "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other ideas about the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above atx-buildings has worked on mapping and loading of most of the addresses for austin osm. Their wiki page is http://wiki.openstreetmap.org/wiki/Austin,_TX/Buildings_Import and github repo is https://github.com/atx-osg/atx-buildings. Their scripts have jsons for converting and cleaning up of street addresses and zipcodes. Since the team standardized the data loading, our dataset was pretty clean to begin with. If we have similar structured data loading with a few rules and clean up scripts, then osm data would become robust. The problem with this however is that too many rules/madatory fields could deter users from contributing more. As an incentive, osm page should display top individual contributors and teams who are working on loading and cleaning of data. \n",
    "\n",
    "Another suggestion would be to indicate areas on the map that have less or incomplete data so that contributors can focus on that region to make the map more complete. There is a lot of missing data for 'city' field in addresses. Using Geopy package, some of the missing information can be filled in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After the review of Austin's OSM data, although incomplete, I believe it has been cleaned well for the purposes of this exercise. The scripts developed during this project was successful in parsing and cleaning most of the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
